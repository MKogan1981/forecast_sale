{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import defaultdict, Counter\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # определение устройства на котором будут производиться рассчеты\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    # информация о данных и путях\n",
    "    pathToFile_csv = 'learning_itemData_172_week_2021-03-31 .csv',\n",
    "    vectorizer_file = \"vectorizer.json\",\n",
    "    model_state_file = \"model.pth\",\n",
    "    save_dir = \"C:\\\\Users\\\\MKoga\\\\MyProgramms\\\\BusinessAnalysis\",\n",
    "    train_proportion = 0.75,\n",
    "    val_proportion = 0.15,\n",
    "    test_proportion = 0.1,\n",
    "    # Гиперпараметры модели\n",
    "    rnn_hidden_size = 180,\n",
    "    # hidden_dim = ...\n",
    "    # Гиперпараметры обучения\n",
    "    seed = 1234,\n",
    "    num_epochs = 100,\n",
    "    learning_rate = 8e-5,\n",
    "    num_layers = 2,\n",
    "    batch_size = 32,\n",
    "    logNormToNormData = lambda x : np.log(x + np.exp(1)),\n",
    "    normToLogNormData = lambda x : np.exp(x) - np.exp(1),\n",
    "\n",
    ")\n",
    "\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерировать новые данные на базе существующих. Добавлением шума (случайные сдвиги продаж на 1 еденицу +-). Перестановка недель (и пар ближайших недель) продаж. Разыгрывание переставляемых недель с вероятностью уменьшающейся от растояния в неделях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N(μ, σ^2)  σ * np.matlib.randn() + μ\n",
    "noiseMatrix = ((0.9 * np.matlib.randn(53)))\n",
    "noiseMatrix = np.floor(np.resize(noiseMatrix, (53,)) + 0.5).astype(np.int32)\n",
    "print(noiseMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "position = np.random.randint(1, 54)\n",
    "nElements = np.random.randint(1, 4)\n",
    "shift = (1.2 * np.matlib.randn(1))\n",
    "shift = np.floor(np.resize(shift, (1,)) + 0.5).astype(np.int32)[0]\n",
    "print(position, nElements, shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class itemsDataset(Dataset):\n",
    "    def __init__(self, data_df):\n",
    "        \"\"\"\n",
    "        Аргументы:\n",
    "            data_df (pandas.DataFrame)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_in_df = data_df[0] \n",
    "        self.data_out_df = data_df[1]\n",
    "        \n",
    "        self.numData_Substring = len(['pseudoRemainder', 'pseudoSales'])\n",
    "        \n",
    "        self.train_in_df = self.data_in_df[self.data_in_df.split == 'train']\n",
    "        self.train_out_df = self.data_out_df[self.data_out_df.split == 'train']\n",
    "        self.train_size = int(len(self.train_out_df)) \n",
    "        #print('train in', self.train_size)\n",
    "        \n",
    "        self.val_in_df  = self.data_in_df[self.data_in_df.split == 'val']\n",
    "        self.val_out_df  = self.data_out_df[self.data_out_df.split == 'val']\n",
    "        self.validation_size = int(len(self.val_out_df))\n",
    "        #print(self.validation_size)\n",
    "        \n",
    "        self.test_in_df = self.data_in_df[self.data_in_df.split == 'test']\n",
    "        self.test_out_df = self.data_out_df[self.data_out_df.split == 'test']\n",
    "        self.test_size = int(len(self.test_out_df))\n",
    "        #print(self.test_size)\n",
    "        \n",
    "        self._lookup_dict = {'train' : (self.train_in_df, self.train_out_df, self.train_size), \n",
    "                             'val' : (self.val_in_df, self.val_out_df, self.validation_size), \n",
    "                             'test' : (self.test_in_df, self.test_out_df, self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset(cls, pathToFile_csv):\n",
    "        # Получаем данные и создаем обучающий (train), проверочный (val) и контрольный (test) фрагменты\n",
    "        itemData= pd.read_csv(args.pathToFile_csv, header= None, sep= '\\s+', encoding= 'cp1251')\n",
    "        # Подготовка данных preparation data\n",
    "        itemData= np.array(itemData)\n",
    "        listItem = itemData[:, 0]\n",
    "        \n",
    "        for i, item in enumerate(listItem): \n",
    "            if item == \"None\":\n",
    "                itemData = np.delete(itemData, i, 0)\n",
    "                listItem = np.delete(listItem, i, 0)\n",
    "                print(i, item)  \n",
    "                       \n",
    "                \n",
    "        # Получение информации о сбалансированности данных \n",
    "        # всего на вход 156 недели из них используем 172-52 = 120  (120 + 52 сдвиг для формирования новых данных)\n",
    "        allWeek = round((itemData.shape[1] - 1) / 3)\n",
    "        yearByWeek = 52 \n",
    "        useNumWeek = allWeek - yearByWeek \n",
    "        print('allWeek {}, useNumWeek {}'.format(allWeek, useNumWeek))\n",
    "        rem = np.zeros((itemData.shape[0], allWeek), dtype=np.float32)\n",
    "        salesData = np.zeros((itemData.shape[0], allWeek), dtype=np.float32) \n",
    "        itemAverageSales = np.zeros(itemData.shape[0], dtype=np.float32)\n",
    "        for item in range(itemData.shape[0]): \n",
    "            for i in range(allWeek):\n",
    "                rem[item, i] = (0.4 * itemData[item, 3*i + 2] + 0.6 * itemData[item, 3*i+ 3])\n",
    "                tempRem = rem[item, i] if rem[item, i] >= 1. else 1.\n",
    "                salesData[item, i] = (itemData[item, 3*i + 1] / np.log10(tempRem + 1.))\n",
    "        \n",
    "        itemAverageSales =  np.mean(salesData, axis=1) \n",
    "        print(itemAverageSales.shape)\n",
    "        bins = np.array([0.3, 0.9, 3, 8, 10000])\n",
    "        nDataInBin = np.zeros(len(bins), dtype=np.int32)\n",
    "        temp = np.zeros(len(bins), dtype=np.int32)\n",
    "        for i in range(len(bins)):\n",
    "            nDataInBin[i] = (itemAverageSales <= bins[i]).sum()\n",
    "            temp[i] = nDataInBin[i]\n",
    "        for i in range(len(bins)):\n",
    "            if i == 0:\n",
    "                print('[ средние продажи в интервале %.3f - %.3f ]  - товаров %5d' %(0, bins[i], nDataInBin[i]))\n",
    "            else:\n",
    "                nDataInBin[i] -= temp[i-1]\n",
    "                print('[ средние продажи в интервале %.3f - %.3f ]  - товаров %5d' %(bins[i-1], bins[i], nDataInBin[i]))        \n",
    "        \n",
    "        # --------------------------------------------------    \n",
    "\n",
    "        # выравниваем группы данных (разбивка по среднем продажам) увеличивая набор данных (аугментация)\n",
    "        \n",
    "        # nSliceToBin = np.array([0, 1, 1, 1, 2]) \n",
    "        # при таком выборе времянных рядов вход одних не может быть выходом других\n",
    "        nSliceToChoice = np.array([0, 1, 1, 1, 1])\n",
    "        \n",
    "        quantityData = np.sum([nSliceToChoice[i] * nDataInBin[i] for i in range(len(nSliceToChoice))])\n",
    "        print('quantity of example - %5d' %quantityData)\n",
    "                \n",
    "        inRemainderData = np.zeros((quantityData, useNumWeek), dtype=np.float32)\n",
    "        inItemSalesData = np.zeros((quantityData, useNumWeek), dtype=np.float32)\n",
    "        # выходные данные на 120 (useNumWeek) недели\n",
    "        # всего 172 (allWeek) недели из них выбираем периоды в 120 недели\n",
    "        remainderData = np.zeros((quantityData, useNumWeek), dtype=np.float32)\n",
    "        itemSalesData = np.zeros((quantityData, useNumWeek), dtype=np.float32)\n",
    "        meanSaleByNumberWeekInYear = np.zeros((quantityData, useNumWeek), dtype=np.float32)\n",
    "        print(itemData.shape, itemSalesData.shape, remainderData.shape, inItemSalesData.shape, inRemainderData.shape) \n",
    "        listItemIndex = []\n",
    "        for itemNum in range(itemData.shape[0]):\n",
    "            rateIndex = 0\n",
    "            for i in range(len(bins)):\n",
    "                if itemAverageSales[itemNum] <= bins[i]:\n",
    "                    rateIndex = i\n",
    "                    break\n",
    "            if nSliceToChoice[rateIndex] != 0:\n",
    "                listItemIndex.append(itemNum)\n",
    "        row = np.array(listItemIndex)\n",
    "        col = np.array([i for i in range(itemData.shape[1])])\n",
    "        itemAverageSales = itemAverageSales[row]\n",
    "        listItem = listItem[row]\n",
    "        itemData = itemData[row[:, np.newaxis], col]\n",
    "        print(itemData.shape, listItem)\n",
    "        \n",
    "        listItem = listItem.tolist() \n",
    "        index = 0\n",
    "        \n",
    "        # заполняем массив усредненными по годам недельными продажами\n",
    "        meanSaleByWeekTemp = np.zeros(salesData.shape, dtype=np.float32)\n",
    "        numWeekForSeasonal = salesData.shape[1] - 16\n",
    "        \n",
    "        def remove_trend(data):\n",
    "            \"\"\"\n",
    "               in data - (2-dim massive, row - article, col - sales by week)\n",
    "               calculate trend use week 0 - 10 and 145 - 155\n",
    "               out data - data without trend\n",
    "            \"\"\"\n",
    "            col = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]\n",
    "            for row in range(data.shape[0]):        \n",
    "                fp = np.polyfit(col, data[row][col], 1)\n",
    "                for x in range(data.shape[1]):\n",
    "                    data[row, x] = data[row, x] - fp[0] * (x - 77)\n",
    "            return data\n",
    "\n",
    "        salesData_remove_trend = remove_trend(salesData.copy())\n",
    "        for index in range(salesData.shape[0]):\n",
    "            for iWeek in range(salesData.shape[1]):\n",
    "                counter = 0\n",
    "                x = 0.\n",
    "                if salesData_remove_trend[index, iWeek % numWeekForSeasonal] >= 0.:\n",
    "                    x += salesData_remove_trend[index, iWeek % numWeekForSeasonal]\n",
    "                    counter += 1           \n",
    "                if salesData_remove_trend[index, (iWeek + 52) % numWeekForSeasonal] >= 0.:\n",
    "                    x += salesData_remove_trend[index, (iWeek + 52) % numWeekForSeasonal]\n",
    "                    counter += 1\n",
    "                if salesData_remove_trend[index, (iWeek + 104) % numWeekForSeasonal] >= 0.:\n",
    "                    x += salesData_remove_trend[index, (iWeek + 104) % numWeekForSeasonal]\n",
    "                    counter += 1              \n",
    "                if counter > 0:    \n",
    "                    meanSaleByWeekTemp[index, iWeek] = x / counter\n",
    "                else:\n",
    "                    meanSaleByWeekTemp[index, iWeek] = -1\n",
    "        # ----------------------------------------------------------------\n",
    "        \n",
    "        # делаем случайные сдвиги данных (у нас массив значений 172, а мы формируем данные по 120 значения)\n",
    "        print('делаем случайные сдвиги данных формируя входные данные размером - {}'.format(useNumWeek)) \n",
    "        index = 0\n",
    "        for item in range(itemData.shape[0]):\n",
    "            startIndex = []\n",
    "            rateIndex = 0\n",
    "            for i in range(len(bins)):\n",
    "                if itemAverageSales[item] <= bins[i]:\n",
    "                    rateIndex = i\n",
    "                    break           \n",
    "#             if nSliceToBin[rateIndex] == 2:\n",
    "#                 startIndex = np.random.choice(52, size = (2,)) + 1\n",
    "#             elif nSliceToBin[rateIndex] == 8:\n",
    "#                 startIndex = np.random.choice(52, size = (8,)) + 1\n",
    "#             elif nSliceToBin[rateIndex] == 4:\n",
    "            startIndex = np.random.choice(52, size = (1,)) + 1        \n",
    "            for iSlice in range(nSliceToChoice[rateIndex]):\n",
    "                if iSlice != 0:\n",
    "                    itemName = listItem[item] + '_slice_' + str(iSlice)\n",
    "                    listItem.append(itemName) \n",
    "                startIndexInItemData = startIndex[iSlice]        \n",
    "                for i in range(inItemSalesData.shape[1]):\n",
    "                    inRemainderData[index, i] = (0.4 * itemData[item, 3*(i+startIndexInItemData-1) + 2] +\n",
    "                        0.6 * itemData[item, 3*(i+startIndexInItemData-1) + 3])\n",
    "                    tempRemainderData = inRemainderData[index, i] if inRemainderData[index, i] >= 1. else 1.\n",
    "                    inItemSalesData[index, i] = (itemData[item, 3*(i+startIndexInItemData-1) + 1] / \n",
    "                        np.log10(tempRemainderData + 1.))                   \n",
    "                for i in range(itemSalesData.shape[1]): \n",
    "                    j = i + startIndexInItemData - 1\n",
    "                    remainderData[index, i] = 0.4 * itemData[item, 3*j+2] + 0.6 * itemData[item, 3*j+3]\n",
    "                    tempRemainderData= remainderData[index, i] if remainderData[index, i] >= 1. else 1.\n",
    "                    itemSalesData[index, i] = itemData[item, 3*j+1] / np.log10(tempRemainderData + 1.)\n",
    "                    meanSaleByNumberWeekInYear[index, i] = meanSaleByWeekTemp[item, j]\n",
    "                index += 1\n",
    "        itemAverageSales = np.zeros(quantityData, dtype=np.float32)        \n",
    "        for i in range(itemSalesData.shape[0]):\n",
    "            itemAverageSales[i] = np.mean(itemSalesData[i][itemSalesData[i] >= 0]) \n",
    "        print('len listItem - ', len(listItem), '   shape itemSalesData - ', itemSalesData.shape)\n",
    "        # ---------------------------------------------------------------\n",
    "        \n",
    "        # Увеличиваем средние продажи при помощи параллельного переноса временного ряда \n",
    "        # на разыгрываемое значение средних продаж\n",
    "        print(\"\"\" Увеличиваем средние продажи при помощи параллельного переноса временного ряда \n",
    "        на разыгрываемое значение средних продаж \"\"\")\n",
    "        change_mean_rt4 = np.arange(0, 201, 2)\n",
    "        change_mean_rt3 = np.arange(0, 101, 1)\n",
    "        probably = np.array([0.015 - np.abs(k) / 5000 for k in range(-50, 51)]) / 1.005\n",
    "        for item in range(itemSalesData.shape[0]):\n",
    "            startIndex = []\n",
    "            rateIndex = 0\n",
    "            for i in range(len(bins)):\n",
    "                if itemAverageSales[item] <= bins[i]:\n",
    "                    rateIndex = i\n",
    "                    break\n",
    "            if rateIndex == 4 or rateIndex == 3:\n",
    "                change_mean = change_mean_rt4 if rateIndex == 4 else change_mean_rt3\n",
    "                mean_sale_value = np.random.choice(change_mean, size = (1,), p = probably) / 10.\n",
    "#                 print(mean_sale_value)\n",
    "                itemName = listItem[item] + '_add_mean'\n",
    "                listItem.append(itemName)\n",
    "                inItemSalesData_new = inItemSalesData[item] + mean_sale_value\n",
    "                inItemSalesData_new[inItemSalesData_new < 0] = 0.\n",
    "                inItemSalesData = np.vstack([inItemSalesData, inItemSalesData_new])       \n",
    "                itemSalesData_new = itemSalesData[item] + mean_sale_value\n",
    "                itemSalesData_new[itemSalesData_new < 0] = 0\n",
    "                itemSalesData = np.vstack([itemSalesData, itemSalesData_new])\n",
    "                meanSaleByNumberWeekInYear_new = meanSaleByNumberWeekInYear[item] + mean_sale_value\n",
    "                meanSaleByNumberWeekInYear_new[meanSaleByNumberWeekInYear_new < 0] = 0\n",
    "                meanSaleByNumberWeekInYear = np.vstack([meanSaleByNumberWeekInYear, meanSaleByNumberWeekInYear_new])\n",
    "        itemAverageSales = np.zeros(itemSalesData.shape[0], dtype=np.float32)        \n",
    "        for i in range(itemSalesData.shape[0]):\n",
    "            itemAverageSales[i] = np.mean(itemSalesData[i][itemSalesData[i] >= 0]) \n",
    "        print('len listItem - ', len(listItem), '   shape itemSalesData - ', itemSalesData.shape)\n",
    "        # -------------------------------------------------------------------------------\n",
    "        \n",
    "        # добавляем данные - отражение временного ряда (изменяем тренд на противоположный)\n",
    "        print('добавляем отраженные данные')\n",
    "        for item in range(itemSalesData.shape[0]):\n",
    "            itemName = listItem[item] + '_flipping'\n",
    "            listItem.append(itemName)     \n",
    "            inItemSalesData_new = np.flip(inItemSalesData[item])\n",
    "            inItemSalesData = np.vstack([inItemSalesData, inItemSalesData_new])\n",
    "            itemSalesData_new = np.flip(itemSalesData[item])\n",
    "            itemSalesData = np.vstack([itemSalesData, itemSalesData_new])\n",
    "            meanSaleByNumberWeekInYear_new = np.flip(meanSaleByNumberWeekInYear[item])\n",
    "            meanSaleByNumberWeekInYear = np.vstack([meanSaleByNumberWeekInYear, meanSaleByNumberWeekInYear_new])\n",
    "        # -------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        itemAverageSales = np.zeros(itemSalesData.shape[0], dtype=np.float32)        \n",
    "        for i in range(itemSalesData.shape[0]):\n",
    "            itemAverageSales[i] = np.mean(itemSalesData[i][itemSalesData[i] >= 0]) \n",
    "        print('len listItem - ', len(listItem), '   shape itemSalesData - ', itemSalesData.shape)\n",
    "        \n",
    "        # добавляем шум в данные по продажам\n",
    "        print('добавляем шум')\n",
    "        for item in range(itemSalesData.shape[0]):\n",
    "            for i in range(len(bins)):\n",
    "                if itemAverageSales[item] <= bins[i]:\n",
    "                    rateIndex = i\n",
    "                    break  \n",
    "            if rateIndex > 2:\n",
    "                itemName = listItem[item] + '_with_noise'\n",
    "                listItem.append(itemName) \n",
    "                # N(μ, σ^2)  σ * np.matlib.randn() + μ\n",
    "                noiseMatrix = ((0.9 * np.matlib.randn(inItemSalesData.shape[1])))\n",
    "                noiseMatrix = np.floor(np.resize(noiseMatrix, (inItemSalesData.shape[1],)) + 0.5).astype(np.int32)\n",
    "                inItemSalesData_new = inItemSalesData[item] + noiseMatrix\n",
    "                inItemSalesData_new = np.where(inItemSalesData_new < 0, 0, inItemSalesData_new)\n",
    "#                 inRemainderData = np.vstack([inRemainderData, inRemainderData[item]])\n",
    "                inItemSalesData = np.vstack([inItemSalesData, inItemSalesData_new])\n",
    "                itemSalesData = np.vstack([itemSalesData, itemSalesData[item]])\n",
    "                meanSaleByNumberWeekInYear = np.vstack([meanSaleByNumberWeekInYear, meanSaleByNumberWeekInYear[item]])\n",
    "                #print(inItemSalesData[item], noiseMatrix, inItemSalesData_new) \n",
    "        itemAverageSales = np.zeros(itemSalesData.shape[0], dtype=np.float32)        \n",
    "        for i in range(itemSalesData.shape[0]):\n",
    "            itemAverageSales[i] = np.mean(itemSalesData[i][itemSalesData[i] >= 0])\n",
    "        print('len listItem - ', len(listItem), '   shape itemSalesData - ', itemSalesData.shape)\n",
    "        # -----------------------------------------------\n",
    "        \n",
    "        # используем скользящую медиану с окном шириной 3 для представления входных данных\n",
    "        print('сглаживаем временной ряд скользящей медианой с окном шириной 3')\n",
    "        def skate_median(series, wide_window):\n",
    "            skate_median_series = []\n",
    "            for_first_median = np.sort([series[0], series[1], 3 * series[1] - 2 * series[2]])\n",
    "#             print(for_first_median)\n",
    "            for_last_median = np.sort([series[-1], series[-2], 3 * series[-2] - 2 * series[-3]])\n",
    "            skate_median_series.append(get_median(for_first_median))\n",
    "            cur_series = [series[i] for i in range(wide_window)]\n",
    "            cur_series = np.array(cur_series)\n",
    "            cur_series = np.sort(cur_series)\n",
    "#             print(cur_series)\n",
    "            n = wide_window\n",
    "            skate_median_series.append(get_median(cur_series))\n",
    "            while n < len(series):\n",
    "                temp = []\n",
    "                data_bool = 0 \n",
    "                for i, value in enumerate(cur_series):\n",
    "                    if value != series[n - wide_window]:\n",
    "                        temp.append(value)\n",
    "                    else:\n",
    "                        if data_bool:\n",
    "                            temp.append(value)             \n",
    "                        else:\n",
    "                            data_bool = 1\n",
    "                place = get_place_insert(temp, series[n]) \n",
    "                cur_series[place] = series[n]\n",
    "                for i, value in enumerate(temp):            \n",
    "                    if i < place:\n",
    "                        cur_series[i] = value\n",
    "                    else:\n",
    "                        cur_series[i+1] = value\n",
    "#                 print(cur_series)        \n",
    "                skate_median_series.append(get_median(cur_series))                              \n",
    "                n += 1 \n",
    "#             print(for_last_median)    \n",
    "            skate_median_series.append(get_median(for_last_median))                           \n",
    "            return skate_median_series\n",
    "\n",
    "        def get_median(series):\n",
    "            n = len(series)\n",
    "            if n % 2 == 0:\n",
    "                return series[n//2 - 1] + series[n//2]\n",
    "            elif n % 2 == 1:\n",
    "                return series[n//2]\n",
    "    \n",
    "        def get_place_insert(series, value):\n",
    "            left = -1\n",
    "            right = len(series)\n",
    "            while right > left + 1:\n",
    "                middle = (right + left) // 2\n",
    "                if series[middle] > value:\n",
    "                    right = middle                \n",
    "                elif series[middle] <= value:\n",
    "                    left = middle\n",
    "            return right \n",
    "#         for i in range(itemSalesData.shape[0]):\n",
    "#             inItemSalesData[i, :] = skate_median(inItemSalesData[i, :], 3)\n",
    "#             itemSalesData[i, :] = skate_median(itemSalesData[i, :], 3)\n",
    "#             meanSaleByNumberWeekInYear[i, :] = skate_median(meanSaleByNumberWeekInYear[i, :], 3)\n",
    "        # ------------------------------------------------------------------------\n",
    "        \n",
    "        # усреднение по трем точкам \n",
    "        print('усреднение по трем точкам')\n",
    "        tSmoothingStart = time()\n",
    "        for i in range(itemSalesData.shape[0]):\n",
    "            tempData = (inItemSalesData[i, :], itemSalesData[i, :])#, inRemainderData[i, :])\n",
    "            tempInDataSales = np.empty_like(tempData[0])\n",
    "            tempDataSales = np.empty_like(tempData[1])\n",
    "#             tempDataRemainder = np.empty_like(tempData[2])\n",
    "            inLenData = inItemSalesData.shape[1]\n",
    "            lenData = itemSalesData.shape[1]\n",
    "            for iweek in range(inLenData):\n",
    "                if iweek == 0:\n",
    "                    tempInDataSales[iweek] = (tempData[0][iweek] + tempData[0][iweek+1]) / 2.   \n",
    "#                     tempDataRemainder[iweek] = (tempData[2][iweek] + tempData[2][iweek+1]) / 2.\n",
    "                    tempDataSales[iweek] = (tempData[1][iweek] + tempData[1][iweek + 1]) / 2.\n",
    "                elif iweek == (inLenData - 1):\n",
    "                    tempInDataSales[iweek] = (tempData[0][iweek-1] + tempData[0][iweek]) / 2.   \n",
    "#                     tempDataRemainder[iweek] = (tempData[2][iweek-1] + tempData[2][iweek]) / 2. \n",
    "                    tempDataSales[iweek] = (tempData[1][iweek-1] + tempData[1][iweek]) / 2.\n",
    "                else:    \n",
    "                    tempInDataSales[iweek] = (tempData[0][iweek-1] + tempData[0][iweek] + tempData[0][iweek+1]) / 3.   \n",
    "#                     tempDataRemainder[iweek] = (tempData[2][iweek-1] + tempData[2][iweek] + tempData[2][iweek+1]) / 3. \n",
    "                    tempDataSales[iweek] = (tempData[1][iweek-1] + tempData[1][iweek] + tempData[1][iweek+1]) / 3. \n",
    "            inItemSalesData[i, :] = tempInDataSales[:]\n",
    "            itemSalesData[i, :] = tempDataSales[:]\n",
    "#             inRemainderData[i, :] = tempDataRemainder[:]                                        \n",
    "#         tSmoothingEnd = time()              \n",
    "        # ---------------------------------\n",
    "        \n",
    "        # добавляем перестановки соседних данных по продажам\n",
    "        print('добавляем перестановки соседних данных')\n",
    "        for item in range(itemSalesData.shape[0]):\n",
    "            for i in range(len(bins)):\n",
    "                if itemAverageSales[item] <= bins[i]:\n",
    "                    rateIndex = i\n",
    "                    break \n",
    "            if rateIndex > 2:\n",
    "                itemName = listItem[item] + '_with_shift'\n",
    "                listItem.append(itemName)\n",
    "                pos = np.random.randint(1, inItemSalesData.shape[1] + 1)\n",
    "                nElements = np.random.randint(1, 4)\n",
    "                shift = (1.2 * np.matlib.randn(1))\n",
    "                shift = np.floor(np.resize(shift, (1,)) + 0.5).astype(np.int32)[0]\n",
    "                inItemSalesData_new = inItemSalesData[item].copy()\n",
    "#                 inRemainderData_new = inRemainderData[item].copy()\n",
    "                for n in range(nElements):\n",
    "                    curPos = pos+n if pos+n < inItemSalesData.shape[1] else pos+n-inItemSalesData.shape[1]\n",
    "                    newPos = pos+shift+n if pos+shift+n < inItemSalesData.shape[1] else pos+shift+n-inItemSalesData.shape[1]\n",
    "                    inItemSalesData_new[curPos] = inItemSalesData[item][newPos]\n",
    "                    inItemSalesData_new[newPos] = inItemSalesData[item][curPos]\n",
    "#                     inRemainderData_new[curPos] = inRemainderData[item][newPos]\n",
    "#                     inRemainderData_new[newPos] = inRemainderData[item][curPos]\n",
    "#                 inRemainderData = np.vstack([inRemainderData, inRemainderData_new])             \n",
    "                inItemSalesData = np.vstack([inItemSalesData, inItemSalesData_new])\n",
    "                itemSalesData = np.vstack([itemSalesData, itemSalesData[item]])\n",
    "                meanSaleByNumberWeekInYear = np.vstack([meanSaleByNumberWeekInYear, meanSaleByNumberWeekInYear[item]])\n",
    "\n",
    "        print('len listItem - ', len(listItem), '   shape itemSalesData - ', itemSalesData.shape) \n",
    "        # --------------------------------------------------------------------------------------                \n",
    "\n",
    "        # убираем часть данных по продажам\n",
    "        print('убираем часть данных')\n",
    "        itemAverageSales = np.zeros(inItemSalesData.shape[0], dtype=np.float32)\n",
    "        for i in range(itemSalesData.shape[0]):\n",
    "            choiceData = inItemSalesData[i, :] >= 0\n",
    "            sumChoiceData = choiceData.sum()\n",
    "            itemAverageSales[i] = inItemSalesData[i][choiceData].mean() if sumChoiceData > 1 else 0.\n",
    "        rateBin = np.array([0, 1, 1, 3, 5]) \n",
    "        for item in range(itemSalesData.shape[0]):\n",
    "            for i in range(len(bins)):\n",
    "                if itemAverageSales[item] <= bins[i]:\n",
    "                    rateIndex = i\n",
    "                    break       \n",
    "            for _ in range(rateBin[rateIndex]):\n",
    "                listWeek = set()\n",
    "                numChangePoint = np.random.randint(0, 5)\n",
    "                if numChangePoint != 0:\n",
    "                    inItemSalesData_new = inItemSalesData[item, :].copy()\n",
    "                    itemSalesData_new = itemSalesData[item, :].copy()\n",
    "                    meanSaleByNumberWeekInYear_new = meanSaleByNumberWeekInYear[item, :].copy()\n",
    "                    posChangePoint = np.random.randint(0, 104, (numChangePoint,))\n",
    "                    numChangeWeek = np.random.randint(0, 4, (numChangePoint,))\n",
    "                    for k in range(numChangePoint):\n",
    "                        for nWeek in range(numChangeWeek[k]):\n",
    "                            inItemSalesData_new[(posChangePoint[k] + nWeek) % 104] = -1.\n",
    "                            listWeek.add((posChangePoint[k] + nWeek) % 104)\n",
    "                    for iWeek in listWeek:\n",
    "                        if ((iWeek + 52) % 104 in listWeek):\n",
    "                            meanSaleByNumberWeekInYear_new[iWeek] = -1.\n",
    "                    itemName = listItem[item] + '_with_reposition'\n",
    "                    listItem.append(itemName)\n",
    "                    inItemSalesData = np.vstack([inItemSalesData, inItemSalesData_new])\n",
    "                    itemSalesData = np.vstack([itemSalesData, itemSalesData_new])\n",
    "                    meanSaleByNumberWeekInYear = np.vstack([meanSaleByNumberWeekInYear, meanSaleByNumberWeekInYear_new])        \n",
    "        print('len listItem - ', len(listItem), '   shape inItemSalesData - ', inItemSalesData.shape) \n",
    "        # -------------------------------------- \n",
    "        \n",
    "        # проверка наполнения групп по продажам\n",
    "        print('проверка наполнения групп по продажам')\n",
    "        itemAverageSales = np.zeros(itemSalesData.shape[0], dtype=np.float32)\n",
    "        for i in range(itemSalesData.shape[0]):\n",
    "            choiceData = itemSalesData[i, :] >= 0\n",
    "            sumChoiceData = choiceData.sum()\n",
    "            itemAverageSales[i] = itemSalesData[i][choiceData].mean() if sumChoiceData > 1 else 0.       \n",
    "        bins = np.array([0., 0.9, 3, 8, 10000]) # первый элемент 0.25 заменен на 0 из-за шума\n",
    "        nDataInBin = np.zeros(len(bins) - 1, dtype=np.int32)\n",
    "        for i in range(len(nDataInBin)):\n",
    "            nDataInBin[i] = (itemAverageSales <= bins[i+1]).sum() - (itemAverageSales < bins[i]).sum()\n",
    "            print('[ средние продажи в интервале %.3f - %.3f ]  - товаров %5d' %(bins[i], bins[i+1], nDataInBin[i])) \n",
    "        # -----------------------------------------                     \n",
    "        \n",
    "        # замена продаж с 1 до 16 недели на -1\n",
    "        print('замена продаж с 1 недели по 16 на -1')\n",
    "        for i in range(itemSalesData.shape[0]):\n",
    "            inItemSalesData[:, :16] = -1.\n",
    "        # ------------------------------------ \n",
    "        \n",
    "        # здесь переход от логнормального распределения к нормальному \n",
    "        inItemSalesData[inItemSalesData != -1.] = args.logNormToNormData(inItemSalesData[inItemSalesData != -1.])\n",
    "        inItemSalesData[inItemSalesData == -1.] = 1e-6\n",
    "        itemSalesData = args.logNormToNormData(itemSalesData)\n",
    "        meanSaleByNumberWeekInYear[meanSaleByNumberWeekInYear != -1.] = \\\n",
    "            args.logNormToNormData(meanSaleByNumberWeekInYear[meanSaleByNumberWeekInYear != -1.])\n",
    "        meanSaleByNumberWeekInYear[meanSaleByNumberWeekInYear == -1.] = 1e-6\n",
    "        # ---------------------------------------------------------- \n",
    "        \n",
    "        # Переверот данных (из будущего в прошлое --> из прошлого в будущее)\n",
    "        inItemSalesData = np.flip(inItemSalesData, axis=1)\n",
    "        itemSalesData = np.flip(itemSalesData, axis=1)\n",
    "        meanSaleByNumberWeekInYear = np.flip(meanSaleByNumberWeekInYear, axis=1)  \n",
    "        # ------------------------------------------------------------------\n",
    "        \n",
    "        listData_in = []\n",
    "        listData_out = []\n",
    "        \n",
    "        for i in range(itemSalesData.shape[0]):            \n",
    "            listData_in.append([meanSaleByNumberWeekInYear[i, :], inItemSalesData[i,:]])\n",
    "            listData_out.append([itemSalesData[i,:]])\n",
    "            \n",
    "        listNumOfItem = np.arange(1, len(listItem)+1)      \n",
    "        dataTable_in = np.array(listData_in)\n",
    "        dataTable_in = dataTable_in.reshape(-1, inItemSalesData.shape[1])\n",
    "        print('dataTable_in.shape - ', dataTable_in.shape)\n",
    "        dataTable_out = np.array(listData_out)\n",
    "        dataTable_out = dataTable_out.reshape(-1, itemSalesData.shape[1])\n",
    "        print('dataTable_out.shape - ', dataTable_out.shape)       \n",
    "        print('NumItem - ', len(listNumOfItem))\n",
    "        iterables_in = [listNumOfItem, ['meanSaleByWeek', 'pseudoSales']]\n",
    "        iterables_out = [listNumOfItem, ['realSales']]\n",
    "        index_in = pd.MultiIndex.from_product(iterables_in, names = ['articles', 'typeData'])\n",
    "        index_out = pd.MultiIndex.from_product(iterables_out, names = ['articles', 'typeData'])      \n",
    "        columns_in = [str(inItemSalesData.shape[1] - i) + ' week ago' for i in range(inItemSalesData.shape[1])]\n",
    "        print('len columns_in -', len(columns_in))\n",
    "        columns_out = [str(itemSalesData.shape[1] - i) + ' week ago' for i in range(itemSalesData.shape[1])] \n",
    "        print('len columns_in -', len(columns_out))\n",
    "        itemSalesTable_in = pd.DataFrame(dataTable_in, index= index_in, columns = columns_in)\n",
    "        itemSalesTable_in = itemSalesTable_in.sort_index(level = 'articles')\n",
    "        display(itemSalesTable_in)\n",
    "        itemSalesTable_out = pd.DataFrame(dataTable_out, index= index_out, columns = columns_out)\n",
    "        itemSalesTable_out = itemSalesTable_out.sort_index(level = 'articles')\n",
    "        display(itemSalesTable_out)\n",
    "        itemSalesTable_in['split'] = ''\n",
    "        itemSalesTable_out['split'] = ''\n",
    "        n_total = len(set(itemSalesTable_in.reset_index()['articles'].values))\n",
    "        n_train = int(args.train_proportion * n_total + 0.5)\n",
    "        n_val = int(args.val_proportion * n_total + 0.5)\n",
    "        n_test = n_total - n_train - n_val\n",
    "        print(n_total, n_train, n_val, n_test)    \n",
    "        t1 = time()\n",
    "        listTrain_in = n_train*[['train','train']]\n",
    "        listTrain_out = n_train*[['train']]\n",
    "        listVal_in = n_val*[['val','val']]\n",
    "        listVal_out = n_val*[['val']]\n",
    "        listTest_in = n_test*[['test','test']]\n",
    "        listTest_out = n_test*[['test']]\n",
    "        \n",
    "        listValue_in = []\n",
    "        listValue_in.extend(listTrain_in)\n",
    "        listValue_in.extend(listVal_in)\n",
    "        listValue_in.extend(listTest_in)\n",
    "        listValue_in = pd.DataFrame(listValue_in)\n",
    "        print('shape listValue_in - ', listValue_in.shape)\n",
    "        display(listValue_in)\n",
    "        listValue_out = []\n",
    "        listValue_out.extend(listTrain_out)\n",
    "        listValue_out.extend(listVal_out)\n",
    "        listValue_out.extend(listTest_out)\n",
    "        listValue_out = pd.DataFrame(listValue_out)\n",
    "        print('shape listValue_out - ', listValue_out.shape)\n",
    "        \n",
    "        mix_up = np.random.permutation(len(listValue_in))\n",
    "        permuteListItem_in = listValue_in.take(mix_up)\n",
    "        listValue_in = permuteListItem_in\n",
    "#         display(listValue_in)\n",
    "        permuteListItem_out = listValue_out.take(mix_up)\n",
    "        listValue_out = permuteListItem_out\n",
    "#         display(listValue_out)\n",
    "        \n",
    "        t2 = time()\n",
    "        print('t2 - t1 = %f' %(t2-t1))\n",
    "#         print('shape itemSalesTable_in - ', itemSalesTable_in.shape)\n",
    "#         print('shape listValue_in - ', listValue_in.shape)\n",
    "        itemSalesTable_in.loc[idx[:,['meanSaleByWeek']], idx['split']] = listValue_in[:][0].values\n",
    "        itemSalesTable_in.loc[idx[:,['pseudoSales']], idx['split']] = listValue_in[:][1].values\n",
    "        itemSalesTable_out.loc[idx[:,['realSales']], idx['split']] = listValue_out[:][0].values\n",
    "        t3 = time()\n",
    "        print('t3 - t2 = %f' %(t3-t2))\n",
    "        display(itemSalesTable_in)  \n",
    "        display(itemSalesTable_out)\n",
    "        return cls((itemSalesTable_in, itemSalesTable_out))\n",
    "    \n",
    "    def get_vector_data(self, index, tableData_df, data_bool = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if data_bool:\n",
    "            item = tableData_df.index[self.numData_Substring * index][0]\n",
    "            inputData = tableData_df.loc[idx[:, ['meanSaleByWeek', 'pseudoSales']], idx['120 week ago':'1 week ago']]\n",
    "            dataMassive = inputData.loc[idx[item, ['meanSaleByWeek', 'pseudoSales']], idx[:]].values\n",
    "        else :\n",
    "            item = tableData_df.index[index][0]\n",
    "            factData = tableData_df.loc[idx[:, ['realSales']], idx['120 week ago':'1 week ago']]\n",
    "            dataMassive = factData.loc[idx[item, :], idx[:]].values\n",
    "        return dataMassive\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        \"\"\"\n",
    "            Выбор фрагментов набора данных по столбцу из объекта dataframe\n",
    "            Аргументы:\n",
    "                split (str): 'train' (обучающий), 'val' (проверочный), 'test' (контрольный)\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_in_df, self._target_out_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "            Аргументы:\n",
    "                index (int): индекс точки данных\n",
    "                \n",
    "            Возвращает:\n",
    "                словарь признаков (x_data) и метки (y_target) точки данных\n",
    "        \"\"\"\n",
    "        \n",
    "        inputData_vector = self.get_vector_data(index, self._target_in_df, True)\n",
    "        factData_vector = self.get_vector_data(index, self._target_out_df, False)\n",
    "        \n",
    "        return {'x_data' : inputData_vector, 'y_target' : factData_vector}\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = itemsDataset.load_dataset(args.pathToFile_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(args.mean, args.sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def generate_batches(dataset, batch_size, shuffle= True, drop_last= True, device= 'cpu'):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset= dataset, batch_size= batch_size, shuffle= shuffle, drop_last= drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for item, tensor in data_dict.items():\n",
    "            out_data_dict[item] = data_dict[item].to(args.device)\n",
    "        yield out_data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvRecNeuronNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvRecNeuronNet, self).__init__()\n",
    "        self.conv1= nn.Conv1d(in_channels= 2, out_channels= 32, kernel_size= 3, stride= 3)\n",
    "        self.conv1_bn= nn.BatchNorm1d(num_features= 32)\n",
    "        self.conv2= nn.Conv1d(in_channels= 32, out_channels= 48, kernel_size= 5, stride= 3, padding= 2)\n",
    "        self.conv2_bn= nn.BatchNorm1d(num_features= 48)\n",
    "        #self.birnn= nn.GRU(input_size= 30, hidden_size= args.rnn_hidden_size, num_layers= args.num_layers, \\\n",
    "                           #bidirectional=True, batch_first= True)\n",
    "        self.birnn= nn.LSTM(input_size=48, hidden_size= args.rnn_hidden_size, num_layers= args.num_layers, \\\n",
    "                               bidirectional=True, batch_first=True, dropout = 0.9)\n",
    "        self.initial_hidden = torch.zeros(2 * args.num_layers, args.batch_size, args.rnn_hidden_size).to(args.device)\n",
    "        self.initial_cell = torch.zeros(2 * args.num_layers, args.batch_size, args.rnn_hidden_size).to(args.device)\n",
    "        self.fc1= nn.Linear(2 * 14 * args.rnn_hidden_size, 120)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= F.leaky_relu(self.conv1(x))\n",
    "        x= self.conv1_bn(x)\n",
    "        x= F.leaky_relu(self.conv2(x))\n",
    "        x= self.conv2_bn(x)\n",
    "        x= x.permute(0, 2, 1)\n",
    "        x_birnn_out, (x_birnn_h, x_birnn_c) = self.birnn(x, (self.initial_hidden, self.initial_cell))\n",
    "        batch_size, seq_size, feat_size = x_birnn_out.shape\n",
    "        x_birnn= x_birnn_out.contiguous().view(args.batch_size, seq_size * feat_size).unsqueeze(1)\n",
    "        x= F.leaky_relu(self.fc1(x_birnn))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFuncLossGet(x_vec):\n",
    "    c = [0.9, 1, 1, 1] # coef\n",
    "    b = [0.34, 0.24, 0, 0] # bias\n",
    "    d = [2, 2, 1, 2] # degree\n",
    "    coef = np.zeros_like(x_vec)\n",
    "    bias = np.zeros_like(x_vec)\n",
    "    degree = np.zeros_like(x_vec)\n",
    "    for i in range(x_vec.shape[0]):\n",
    "        for j in range(x_vec.shape[1]):\n",
    "            x = x_vec[i, j]\n",
    "            (coef[i, j], bias[i,j], degree[i, j]) = np.where(x < -1, (c[0], b[0], d[0]), np.where(x < -0.4, \\\n",
    "                                (c[1], b[1], d[1]), np.where(x < 0, (c[2], b[2], d[2]), (c[3], b[3], d[3]))))\n",
    "    return (coef, bias, degree)\n",
    "\n",
    "def PWLLoss(outputs, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "        Кусочно-линейная функция потерь. Штрафуется сильнее значение больше метки\n",
    "    \"\"\"\n",
    "    outputs = outputs.squeeze(1)\n",
    "    labels = labels.squeeze(1)\n",
    "    δ = torch.zeros(outputs.shape, requires_grad=True)\n",
    "    σ = torch.zeros(outputs.shape[0], requires_grad=True)\n",
    "    δ = labels - outputs\n",
    "    c, b, d = myFuncLossGet(δ.to('cpu').detach().numpy())\n",
    "    c = torch.from_numpy(c).to(args.device)\n",
    "    b = torch.from_numpy(b).to(args.device)\n",
    "    d = torch.from_numpy(d).to(args.device)\n",
    "    σ = torch.sum(c * torch.pow(torch.abs(δ), d) + b, dim=1)\n",
    "    return torch.mean(σ) \n",
    "\n",
    "def MyMSELoss(outputs, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "        Квадратичная функция потерь.\n",
    "    \"\"\"\n",
    "    outputs = outputs.squeeze(1)\n",
    "    labels = labels.squeeze(1)\n",
    "    δ = torch.zeros(outputs.shape, requires_grad=True)\n",
    "    σ = torch.zeros(outputs.shape[0], requires_grad=True)\n",
    "    δ = labels - outputs\n",
    "    σ = torch.sum(torch.pow(torch.abs(δ), 2), dim=1)\n",
    "    return torch.mean(σ) \n",
    "\n",
    "def MyMSELossWithPenaltyForSumDiff(outputs, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "        Квадратичная функция потерь с дополнительным штрафом за суммарную разность.\n",
    "    \"\"\"\n",
    "    \n",
    "    def myFuncSumDiffLoss(x):\n",
    "        vec_err = torch.zeros_like(x, requires_grad=True, device = args.device)      \n",
    "        vec_err = torch.abs(torch.pow((torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x)), 1))\n",
    "        return vec_err\n",
    "    outputs = outputs.squeeze(1)\n",
    "    labels = labels.squeeze(1)\n",
    "    δ = torch.zeros(outputs[:, -16].shape, requires_grad=True, device = args.device)\n",
    "    δ_forecast = torch.zeros(outputs[:, -16:].shape, requires_grad=True, device = args.device)\n",
    "    cumsum_δ = torch.zeros((outputs.shape[0], 16), requires_grad=True, device = args.device) \n",
    "    σ = torch.zeros(outputs.shape[0], requires_grad=True, device = args.device)\n",
    "    δ = torch.abs(labels[:, 0:-16] - outputs[:, 0:-16])\n",
    "    δ_forecast = torch.abs(labels[:, -16:] - outputs[:, -16:])\n",
    "    cumsum_δ = torch.abs(torch.cumsum(labels[:, -16:], dim=1) - torch.cumsum(outputs[:, -16:], dim=1))\n",
    "    α, β, γ = 1, 2.5, 0.75\n",
    "    σ = α * torch.sum(torch.pow(δ, 2), dim=1) + β * torch.sum(torch.pow(δ_forecast, 2), dim=1) + \\\n",
    "        γ * torch.abs(torch.sum(myFuncSumDiffLoss(cumsum_δ), dim=1))\n",
    "    return torch.mean(σ)\n",
    "\n",
    "def SumDiffLoss(outputs, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "        Функция потерь с штрафом за суммарную разность.\n",
    "    \"\"\"\n",
    "    outputs = outputs.squeeze(1)\n",
    "    labels = labels.squeeze(1)\n",
    "    δ = torch.zeros(outputs[:, -16].shape, requires_grad=True, device = args.device)\n",
    "    δ_forecast = torch.zeros(outputs[:, -16:].shape, requires_grad=True, device = args.device)\n",
    "    cumsum_δ = torch.zeros((outputs.shape[0], 16), requires_grad=True, device = args.device)\n",
    "    σ = torch.zeros(outputs.shape[0], requires_grad=True, device = args.device)\n",
    "    δ = torch.abs(labels[:, 0:-16] - outputs[:, 0:-16])\n",
    "    δ_forecast = torch.abs(labels[:, -16:] - outputs[:, -16:])\n",
    "    cumsum_δ = torch.abs(torch.cumsum(labels[:, -16:], dim=1) - torch.cumsum(outputs[:, -16:], dim=1))\n",
    "#     norm_cumsum = torch.arange(1, cumsum_δ.shape[1]+1, device = args.device)\n",
    "    norm_cumsum = torch.tensor([0.2, 0.4, 0.6, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], device = args.device)\n",
    "    for i in range(outputs.shape[0]):\n",
    "        cumsum_δ[i, :] = cumsum_δ[i, :] / norm_cumsum\n",
    "    α, β, γ, θ = 1., 3., 1., 3. \n",
    "    σ = α * torch.sum(torch.pow(δ, 2), dim=1) + β * torch.sum(torch.pow(δ_forecast, 2), dim=1) + \\\n",
    "        γ * torch.sum(torch.pow(cumsum_δ, 2), dim=1) + θ * torch.sum(cumsum_δ, dim=1)\n",
    "#     σ = α * torch.sum(torch.pow(δ, 2), dim=1) + β * torch.sum(torch.abs(cumsum_δ), dim=1)\n",
    "    return torch.mean(σ)\n",
    "\n",
    "loss_fn = MyMSELossWithPenaltyForSumDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvRecNeuronNet().to(args.device)\n",
    "# loss_fn = nn.MSELoss()\n",
    "# optimizer= optim.SGD(net.parameters(), lr= args.learning_rate)\n",
    "optimizer = optim.Adam(net.parameters(), lr= args.learning_rate, weight_decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \\\n",
    "u'C:\\\\Users\\\\MKoga\\\\MyProgramms\\\\BusinessAnalysis\\\\develop_forecast\\\\CNN_RNN_forecast_in_combination_with_recovery_new_last_ver3.pt'\n",
    "net.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "# writer.add_graph(net, torch.rand([args.batch_size, 1, args.rnn_hidden_size]).to(args.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_lr(model, loss_fn, optimizer, init_value=1e-5, final_value=1e-3):\n",
    "    batch_generator = generate_batches(dataset, batch_size = args.batch_size, device = args.device)\n",
    "    model.train()\n",
    "    numberOfBatchInEpoch = int(dataset.__len__() / args.batch_size)\n",
    "    print(numberOfBatchInEpoch, dataset.__len__())\n",
    "    update_step =  (final_value / init_value) ** ( 1 / (numberOfBatchInEpoch / 2))\n",
    "    print(update_step)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    best_loss = 0.0\n",
    "    best_lr = init_value\n",
    "    losses = []\n",
    "    running_loss= 0.0\n",
    "    log_lrs = []\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        \n",
    "        inputs, labels = batch_dict['x_data'].float(), batch_dict['y_target'].float() \n",
    "        # print(batch_dict['x_data'].shape)\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        writer.add_scalar('loss', loss, batch_index)\n",
    "        \n",
    "        if batch_index > 1 and loss > 10000 * best_loss:\n",
    "            return log_lrs[10:-5], losses[10:-5], best_lr\n",
    "        if loss < best_loss or batch_index == 1:\n",
    "            best_loss = loss\n",
    "            best_lr = lr\n",
    "            \n",
    "        loss_batch = loss.to('cpu').item()\n",
    "        running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_index < (numberOfBatchInEpoch / 2):\n",
    "            lr *= update_step\n",
    "        else:\n",
    "            lr /= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "        \n",
    "        if batch_index % 20 == 19:\n",
    "            mean_value = np.mean(np.array(losses, dtype=np.float32))\n",
    "            print('[%5d]  (min loss %.4f, mean loss: %.4f,  best lr: %.8f)' %(batch_index + 1, best_loss, \n",
    "                mean_value, best_lr))\n",
    "            ax.clear()\n",
    "            ax.plot(log_lrs[10:-5], losses[10:-5])\n",
    "            ax.relim()\n",
    "            ax.autoscale_view(True, True, True)\n",
    "            fig.canvas.draw()\n",
    "            \n",
    "    return log_lrs[10:-5], losses[10:-5], best_lr, running_loss\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    \n",
    "    dataset.set_split('train')\n",
    "    logs, losses, best_lr, running_loss_train = find_lr(net, loss_fn, optimizer)\n",
    "    plt.plot(logs, losses)\n",
    "    loss_train.append(running_loss_train)\n",
    "    \n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_batches(dataset, batch_size = args.batch_size, device = args.device)\n",
    "    net.eval()\n",
    "    running_loss_val = 0.0\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        y_pred = net(x = batch_dict['x_data'].to(dtype=torch.float32)).float().squeeze(1)\n",
    "        loss= loss_fn(y_pred, batch_dict['y_target'].float().squeeze(1))\n",
    "        loss_batch = loss.to('cpu').item()\n",
    "        running_loss_val += (loss_batch - running_loss_val) / (batch_index + 1)\n",
    "        if batch_index % 5 == 4:\n",
    "            print('[%5d] loss: %.3f' %(batch_index + 1, running_loss_val))\n",
    "    loss_val.append(running_loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train = np.array(loss_train)\n",
    "error_val = np.array(loss_val)\n",
    "epoch_index = np.arange(error_train.size)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.ylabel('Ошибка')\n",
    "plt.xlabel('Номер эпохи') \n",
    "plt.grid(True, axis='y', color='black',  linestyle='dashed')\n",
    "plt.grid(True, axis='x', color='black',  linestyle='dashed')\n",
    "plt.plot(epoch_index, error_train, label = 'train')\n",
    "plt.plot(epoch_index, error_val, label = 'validation')\n",
    "plt.legend()\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \\\n",
    "u'C:\\\\Users\\\\MKoga\\\\MyProgramms\\\\BusinessAnalysis\\\\develop_forecast\\\\CNN_RNN_forecast_in_combination_with_recovery_new_ver5.pt'\n",
    "torch.save(net.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlim(-6.5, -3.5)\n",
    "plt.grid(True, axis='x', color='green',  linestyle='dashed')\n",
    "plt.plot(logs, losses)\n",
    "plt.vlines(-4.9, 0, 50, alpha=0.5, colors='r0', lw=5)\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = u'C:\\\\Users\\\\MKoga\\\\MyProgramms\\\\BusinessAnalysis\\\\reconstactionData\\\\CNN_RNN_recovery_data_201120.pt'\n",
    "net.load_state_dict(torch.load(path))\n",
    "optimizer.param_groups[0][\"lr\"] = args.learning_rate\n",
    "loss_fn = MyMSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)\n",
    "# print(net.conv1.weight)\n",
    "# print(net.birnn.weight_hh_l1.data)\n",
    "print(f'shape of output layer {net.fc1.weight.data.shape}')\n",
    "print(np.argwhere(net.fc1.weight.data[0].to('cpu').abs() > 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_index in range(args.num_epochs):\n",
    "    \n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_batches(dataset, batch_size = args.batch_size, device = args.device)\n",
    "      \n",
    "    running_loss= 0.0\n",
    "    net.train()\n",
    "    \n",
    "    numberOfBatchInEpoch = int(dataset.__len__() / args.batch_size)\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(batch_generator):  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #print(batch_dict['x_data'].float().shape)\n",
    "        y_pred = net(x = batch_dict['x_data'].float())\n",
    "        \n",
    "        \"\"\"\n",
    "        if batch_index == 100:\n",
    "            print(y_pred.shape, batch_dict['x_data'].float().squeeze(1)[:, :-1].shape, \\\n",
    "                  batch_dict['y_target'].float().squeeze(1).shape)\n",
    "            dif = y_pred.float() - batch_dict['y_target'].float().squeeze(1)\n",
    "            print(dif)\n",
    "            #print(y_pred, batch_dict['x_data'].float().squeeze(1)[:, :-1], batch_dict['y_target'].float().squeeze(1))\n",
    "        \"\"\"    \n",
    "        #print(y_pred.shape, batch_dict['y_target'].float().squeeze(1).shape)\n",
    "        loss = loss_fn(y_pred, batch_dict['y_target'].float()) #.squeeze(1))\n",
    "        writer.add_scalar('loss', loss, batch_index + epoch_index * numberOfBatchInEpoch)\n",
    "        print(loss)\n",
    "        loss_batch = loss.to('cpu').item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss_batch\n",
    "\n",
    "        if batch_index % 20 == 19:\n",
    "          print('[%d, %5d] loss: %.3f' %(epoch_index + 1, batch_index + 1, (running_loss / 20)))\n",
    "          running_loss= 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)\n",
    "# print(net.conv1.weight)\n",
    "print(net.birnn.weight_hh_l1.data)\n",
    "print(y_pred.squeeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(net.birnn.weight_hh_l1.data.abs() > 0.16).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvRecNeuronNet().to(args.device)\n",
    "path = u'C:\\\\Users\\\\MKoga\\\\MyProgramms\\\\BusinessAnalysis\\\\reconstactionData\\\\CNN_RNN_recovery_data_find_lr.pt'\n",
    "net.load_state_dict(torch.load(path))\n",
    "loss_fn = PWLLoss\n",
    "# net.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size = args.batch_size, device = args.device)\n",
    "running_loss= 0.0\n",
    "net.eval()\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    y_pred = net(x = batch_dict['x_data'].to(dtype=torch.float32)).float().squeeze(1)\n",
    "#     print('prediction shape - {}; target shape -{}'.format(y_pred.shape, batch_dict['y_target'].shape))\n",
    "    loss= loss_fn(y_pred, batch_dict['y_target'].float().squeeze(1))\n",
    "#     display(loss_fn(y_pred, batch_dict['y_target'].float()))\n",
    "    loss_batch = loss.to('cpu').item()\n",
    "    running_loss += loss_batch\n",
    "    if batch_index % 2 == 1:\n",
    "        print('[%5d] loss: %.3f' %(batch_index + 1, (running_loss / 5)))\n",
    "        running_loss= 0.0\n",
    "        data_in = np.array(batch_dict['x_data'].float()[:, 1, :].squeeze(1).to('cpu'))\n",
    "#         print('data_in.shape', data_in.shape)\n",
    "        dataY = np.array(batch_dict['y_target'].float().squeeze(1).to('cpu'))\n",
    "        y_pred = np.array(y_pred.to('cpu').squeeze(1).detach().numpy())\n",
    "#       здесь переход от нормального распределения к логнормальному \n",
    "        data_in[data_in != 1e-6] = args.normToLogNormData(data_in[data_in != 1e-6]) \n",
    "        data_in[data_in == 1e-6] = -1\n",
    "        dataY = args.normToLogNormData(dataY)\n",
    "        y_pred = args.normToLogNormData(y_pred)\n",
    "        y_pred[y_pred < 0.] = 0.\n",
    "#       -----------------------------------------------------------\n",
    "#       здесь восстановление ранее отнормированных данных -----------------------------------------------------\n",
    "#         data_in[data_in != -10] = rec_fromNorm(data_in[data_in != -10], args.mean, args.sd, args.f_recoveryData)\n",
    "#         data_in[data_in == -10] = -1\n",
    "#         dataY = rec_fromNorm(dataY, args.mean, args.sd, args.f_recoveryData)\n",
    "#         y_pred = rec_fromNorm(y_pred, args.mean, args.sd, args.f_recoveryData)\n",
    "#         y_pred[y_pred < 0.] = 0.\n",
    "#       -------------------------------------------------------------------------------------------------------\n",
    "        for i in range(3):\n",
    "            iData= np.random.randint(0, args.batch_size - 1)\n",
    "            pointForAverage = 1\n",
    "            step = int(pointForAverage / 2)\n",
    "            allWeek = int(data_in.shape[1] / pointForAverage)\n",
    "            allForecastWeek = int(y_pred.shape[1] / pointForAverage)\n",
    "            yPred= np.zeros(allForecastWeek, dtype= np.float32)\n",
    "            yReal= np.zeros(allForecastWeek, dtype= np.float32)\n",
    "            yInData= np.zeros(allWeek, dtype= np.float32)           \n",
    "            for j in range(allForecastWeek):\n",
    "                iterable_pred = (y_pred[iData, j*pointForAverage + k] for k in range(pointForAverage))  \n",
    "                yPred[j] = np.sum(np.fromiter(iterable_pred, dtype= np.float32))\n",
    "                iterable_real = (dataY[iData, j*pointForAverage + k] for k in range(pointForAverage)) \n",
    "                yReal[j] = np.sum(np.fromiter(iterable_real, dtype= np.float32))\n",
    "            for j in range(allWeek):\n",
    "                iterable_yInData = (data_in[iData, j*pointForAverage + k] for k in range(pointForAverage))\n",
    "                saleInIterableBool = [False if data_in[iData, j*pointForAverage + k] < 0 else True \\\n",
    "                              for k in range(pointForAverage)]\n",
    "                yInData[j] = np.sum(np.fromiter(iterable_yInData, dtype= np.float32), where= saleInIterableBool)\n",
    "                if not np.array(saleInIterableBool).any():\n",
    "                    yInData[j] = -1\n",
    "            week_forecast= np.arange(allForecastWeek * pointForAverage + 1, 1 + step, -pointForAverage)\n",
    "            week = np.arange(allForecastWeek*pointForAverage+1+step, (allWeek+allForecastWeek)*pointForAverage+1, \\\n",
    "                    pointForAverage)\n",
    "                                 \n",
    "            plt.rc('font', family='Verdana')\n",
    "            plt.rcParams.update({'font.size': 10, 'figure.figsize' : (8, 6), 'figure.dpi' : 200})  \n",
    "            plt.ioff()\n",
    "            fig= plt.figure()\n",
    "            ax= fig.add_subplot(111)\n",
    "            lab1= u'real sale'\n",
    "            ax.scatter(week_forecast, yReal, color= 'darkblue')\n",
    "            lab2= u'Predict sale'\n",
    "            ax.scatter(week_forecast, yPred, color='red', alpha=0.7)\n",
    "            lab3= u'change in-sale'\n",
    "            ax.scatter(week_forecast, yInData, color= 'green')\n",
    "            ax.legend((lab1, lab2, lab3), frameon=True, loc='best')\n",
    "            ax.set_ylabel('Предсказание')\n",
    "            ax.set_xlabel('Номер недели') \n",
    "            ax.grid(True, axis='y', color='black',  linestyle='dashed')\n",
    "            ax.grid(True, axis='x', color='black',  linestyle='dashed')\n",
    "            ax.set_xlim(allForecastWeek * pointForAverage + 1, 0)\n",
    "#             ax.set_xlim((allWeek + allForecastWeek) * pointForAverage + 1, 0)\n",
    "            ax.set_ylim(-2, np.max([np.max(yInData), np.max(yPred), np.max(yReal)]))\n",
    "            plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_model_makeup(forecasts, facts):\n",
    "    γ = 0.2\n",
    "    err_exp_smooth = np.zeros((forecasts.shape[0], forecasts.shape[1], forecasts.shape[2]), dtype = 'f4')\n",
    "    ω = np.zeros((forecasts.shape[2], forecasts.shape[1]), dtype = 'f4')\n",
    "    def exponential_smoothing(series, alpha):\n",
    "        result = [np.abs(series[0])] # first value is same as series\n",
    "        for n in range(1, len(series)):\n",
    "            result.append(alpha * np.abs(series[n]) + (1 - alpha) * result[n-1])\n",
    "        return np.array(result)\n",
    "    facts = facts[:, :, np.newaxis]\n",
    "    err = np.abs(np.cumsum(forecasts - facts, axis=1))\n",
    "    for i in range(forecasts.shape[0]):\n",
    "        for n_model in range(forecasts.shape[2]):\n",
    "            err_exp_smooth[i, :, n_model] = exponential_smoothing(err[i, :, n_model], γ)\n",
    "    err_exp_smooth = err_exp_smooth.mean(axis = 0)\n",
    "    inverse_ω = np.power(err_exp_smooth, -1)\n",
    "    for j in range(forecasts.shape[2]):\n",
    "        ω[j] = inverse_ω[:, j] / np.sum(inverse_ω, axis = 1).reshape(-1)\n",
    "    return ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('val')\n",
    "batch_generator = generate_batches(dataset, batch_size = args.batch_size, device = args.device)\n",
    "net.eval()\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    y_pred = net(x = batch_dict['x_data'].to(dtype=torch.float32)).float().squeeze(1)\n",
    "    y_pred = np.array(y_pred.to('cpu').squeeze(1).detach().numpy())[:, -16:]\n",
    "    # оценка временного ряда - взвешенная средняя    \n",
    "    weights = np.array([0.06, 0.12, 0.19, 0.27, 0.36])\n",
    "    cur_series = np.array(batch_dict['x_data'].float()[:, 0, -5:].squeeze(1).to('cpu'))\n",
    "    rolling_weights_mean_sale = np.mean(cur_series * weights, axis=1)\n",
    "    rolling_weights_mean_sale = rolling_weights_mean_sale[:, np.newaxis]\n",
    "    for _ in range(15):\n",
    "        if i != 0:\n",
    "            cur_series = np.hstack((cur_series[:, 1:], rolling_weights_mean_sale[:, -1:]))\n",
    "            temp_mean = np.mean(cur_series * weights, axis=1)\n",
    "            temp_mean = temp_mean[:, np.newaxis]\n",
    "            rolling_weights_mean_sale = np.hstack((rolling_weights_mean_sale, temp_mean))\n",
    "        else:\n",
    "            cur_series = np.hstack((cur_series[:, 1:], rolling_weights_mean_sale[:]))\n",
    "            temp_mean = np.mean(cur_series * weights, axis=1)\n",
    "            temp_mean = temp_mean[:, np.newaxis]\n",
    "            rolling_weights_mean_sale = np.hstack((rolling_weights_mean_sale, temp_mean))  \n",
    "    # -------------------------------------------\n",
    "    forecasts = np.dstack((y_pred, rolling_weights_mean_sale))\n",
    "    facts = batch_dict['y_target'].float()[:, :, -16:]\n",
    "    facts = facts.to('cpu').squeeze(1).detach().numpy()\n",
    "#     print(forecasts.shape, facts.shape)\n",
    "    if batch_index == 0:\n",
    "        w = adaptive_model_makeup(forecasts, facts)\n",
    "    else:\n",
    "        w = np.dstack((w, adaptive_model_makeup(forecasts, facts)))\n",
    "    \n",
    "print(np.mean(w, axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_forecast = np.arange(16)\n",
    "err_y_pred = np.abs(np.cumsum(y_pred - facts, axis=1)).mean(axis=0)\n",
    "err_mwa = np.abs(np.cumsum(rolling_weights_mean_sale - facts, axis=1)).mean(axis=0)\n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "lab1= u'cumsum mwa sale'\n",
    "ax1.scatter(week_forecast, err_mwa, color= 'darkblue')\n",
    "lab2= u'cumsum Predict sale'\n",
    "ax1.scatter(week_forecast, err_y_pred, color='red', alpha=0.7)\n",
    "ax1.set_ylabel('Предсказание')\n",
    "ax1.set_xlabel('Номер недели') \n",
    "ax1.grid(True, axis='y', color='black',  linestyle='dashed')\n",
    "ax1.grid(True, axis='x', color='black',  linestyle='dashed')\n",
    "ax1.set_xlim(0, 16)\n",
    "ax1.set_ylim(np.min([err_y_pred, err_mwa]) - 1, np.max([err_y_pred, err_mwa]) + 1)\n",
    "ax1.legend((lab1, lab2), frameon=True, loc='best')\n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
